

Module 2.4 - Networks with Tensors
=====================================


Module 2.4
------------

   Applying Neural Networks


Review
=============


Shape Maniputation
------------------

* Permutation ::

    tensor.permute(1, 0)

.. image:: figs/Tensors/matrix1.png
.. image:: figs/Tensors/matrix2.png

Shape Maniputation
------------------

* Permutation ::

    tensor.permute(2, 1, 0, 3)

Rearranges dims in order given.     

           
How does this work
--------------------

* **Storage** :  1-D array of numbers of length `size`

* **Strides** : tuple that provides the mapping from user `indexing`
  to the `position` in the 1-D `storage`.

Tensor Functions
-----------------

Unary ::

    new_tensor = tensor.log()

Binary (for now, only same shape) ::

    new_tensor = tensor1 + tensor2

Reductions ::

    new_tensor = tensor.sum()

Tensor Ops
-----------------

.. revealjs_fragments::

   1) **Map** - Apply to all elements

   2) **Zip** (same as zipWith) - Apply to all pairs

   3) **Reduce** - Reduce a subset


Zip With Broadcasting
---------------------

.. image:: figs/Ops/zip\ broad\ back.png

Zip With Broadcasting
---------------------

Code ::

  out = zeros(3, 2)
  for i in range(3):
      for j in range(2):
          out[i, j] = a[i] + b[j]

Matrix Scalar Addition
-----------------------

Doesn't Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5])

Does Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5]).view(4, 1)


Applying the Rules
-------------------


* (3, 4, 5) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 5) => X


Map Gradient
------------

.. image:: figs/Ops/map\ back.png
           :align: center

Zip Gradient
------------

.. image:: figs/Ops/zip\ back.png
           :align: center


Reduce Gradient
----------------


.. image:: figs/Ops/reduce\ back.png
           :align: center

Zip Broadcasting
---------------------

.. image:: figs/Ops/zip\ broad.png


Example: Negative
------------------------
.. math::

   f(x) = -x

.. image:: figs/Ops/map.png
           :align: center

                   
Example: Negative Back
------------------------

.. math::

     d \times f'(x) = -d


.. image:: figs/Ops/map\ back.png
           :align: center
                   
Example: Inverse
------------------------

.. math::

     f(x) = 1/x


.. image:: figs/Ops/map.png
           :align: center

Example: Inverse Back
------------------------

.. math::

   d \times f'(x) = d \times -x^2 

.. image:: figs/Ops/map\ back.png
           :align: center


Matrix-vector operations
-------------------------


.. image:: figs/Broadcast/vector.png
           :align: center

Backwards (Expand)
-------------------------


.. image:: figs/Broadcast/vector.png
           :align: center

Backwards (Expand)
-------------------------

Rules:

* You can return gradient for broadcasted input.

* Autodiff will collapse it to the original size. 


Chain Rule 
-------------------------

If a variable is used twice, derivative is the use paths. 
  
.. math::

   \begin{eqnarray*}
   z_1 &=& g(x) \\
   z_2 &=& h(x) \\
   f'_x(g(x), h(x)) &=& g'(x) \times f'_{z_1}(z_1, z_2) + h'(x) \times f'_{z_2}(z_1, z_2)
   \end{eqnarray*}
  
                   
Quiz
------

  `Quiz`_  

  
Outline
----------------

.. revealjs_fragments::

   * Training
   * Simple NLP


Training
=========

Reminder
----------

* Dataset - Data to fit
* Model - Shape of fit
* Loss - Goodness of fit

Linear Model Example
----------------------

* Parameters

.. image:: figs/Graphs/weight.png
           :align: center
           :width: 400px


.. image:: figs/Graphs/bias.png
           :align: center
           :width: 400px

Model: Math
------------

.. math::

   \begin{eqnarray*}
   \text{lin}(x; w, b) &=& x_1 \times w_1 + x_2 \times w_2 + b \\
   h_ 1 &=& \text{ReLU}(\text{lin}(x; w^0, b^0)) \\
   h_ 2 &=& \text{ReLU}(\text{lin}(x; w^1, b^1))\\
   m(x_1, x_2) &=& \text{lin}(h; w, b)
   \end{eqnarray*}

                   


Implementation Tricks
----------------------


* Batching -> Compute the loss on many examples.

* Parameters -> Store all in Tensors.

* Opt -> Decide how to change the parameters
  

Batching
--------

.. image:: figs/NLP/batch.png
           :align: center

Batching
--------

* Extra dimension on all the data.

* Will be very important for future assignments

Parameters
-----------

* Tensors inside the model.
  
* Receive the gradient updates

* `Accumumlated` i.e. summed for each update


Optimizer
---------

* Decide how to update the parameters.
  
* Example

  

Parameter Fitting
------------------------------------

1. Compute the loss function, :math:`L(w_1, w_2, b)`
2. See how small changes would change the loss
3. Update to parameters to locally reduce the loss


  
                   
Loss
------

1) Compute Loss ::

    out = model.forward(X).view(data.N)
    loss = -((out * y) + (out - 1.0) * (y - 1.0)).log()




Model: Code
------------

1) Model ::

    class Network(minitorch.Module):
      def __init__(self):
        ...
        self.layer1 = Linear(2, HIDDEN)
        self.layer2 = Linear(HIDDEN, HIDDEN)
        self.layer3 = Linear(HIDDEN, 1)


Layer 1: Weight
----------------

.. image:: figs/NLP/weight.png
           :align: center


Layer 1: Bias
--------------

.. image:: figs/NLP/bias.png
           :align: center

Key Task
---------

.. revealjs_fragments::

   * Use broadcasting to implement the linear function
   * Hint: Align `batch` x `features` x `hidden` to make it work


Layer 2: Weights
----------------

.. image:: figs/NLP/weight2.png
           :align: center


Compute Derivatives
--------------------

Step 2 ::

  (loss.sum().view(1)).backward()
  print(model.layer1.w_1.value.grad)

.. image:: figs/NLP/weight.png
           :align: center


Layer 1: Weight Grad
--------------------

.. image:: figs/NLP/weight.png
           :align: center

.. image:: figs/NLP/weight.png
           :align: center


Update Parameters
--------------------


Step 3 ::

    for p in model.parameters():
        if p.value.grad is not None:
            p.update(p.value - RATE * (p.value.grad / float(data.N)))

Broadcasting
-------------

.. revealjs_fragments::

   * Batches
   * Loss Computation
   * Linear computation
   * Autodifferentiation
   * Gradient updates


Observations
-------------

.. revealjs_fragments::

   * Exactly the same function as Module-1
   * No loops within tensors



Simple NLP
=============

Context
--------

.. revealjs_fragments::

   * What are dimensions for?
   * What does broadcasting buy us?
   * What are non-spatial dimensions for?

Natural Language Processing
----------------------------

* Systems for human language
* Broad area of study with lots of challenges
* Heavily uses ML, more in recent years



Sentiment Classification
-------------------------

.. revealjs_fragments::

   * Canonical sentence classification problem

   * Given sentence predict sentiment class

   * Key aspects: word polarity


Data
------

.. image:: figs/NLP/negative.png
           :align: center

.. image:: figs/NLP/positive.png
           :align: center



Problem Setup
---------------

.. revealjs_fragments::

   * Training: Exactly the same as simple
   * Loss: Exactly the same as simple
   * Models: Mostly similar to the simple problem.



Modeling Challenges
--------------------

.. revealjs_fragments::

   1) Converting words to tensors
   2) Converting sentences to tensors
   3) Handling word combinations


Word -> Tensors
===============


What is a word?
----------------

.. revealjs_fragments::
   * Treat words as index in vocabulary
   * Represent as a one-hot vector

Challenge1: Vector Form
------------------------

.. image:: figs/NLP/onehot.png
           :align: center


One-Hot Issue
--------------

.. revealjs_fragments::

   * Tens of thousands of words
   * Opposite problem as before, 2-features to 10,000
   * ``Embedding'' represent high-dim space in low dim


Embedding Table
------------------------

.. image:: figs/NLP/embweight.png
           :align: center

Intuition: Lookup in Table
---------------------------

Get word vector ::

  embeddings[word]

* Challenge:  How to compute `backward`


Alternative: Lookup by broadcast
------------------------------------

Get word vector ::

   (embeddings * word_one_hot.view(VOCAB, 1)).sum(0)


Embedding One
------------------------

.. image:: figs/NLP/embone.png
           :align: center

How does this share information?
---------------------------------

.. revealjs_fragments::
   * Similar words have similar embedding dim

   * Dot-product - easy way to tell similarity ::

       (word_emb1 * word_emb2).sum()

   * Differentiable!


Embedding Layer
----------------

Easy to write as a layer ::

  class Embedding(minitorch.Module):
    def __init__(self, vocab_size, emb_size):
        super().__init__()
        self.weights = \
          minitorch.Parameter(minitorch.rand((vocab_size, emb_size))
        self.vocab_size = vocab_size

    def forward(input):
        return (self.weights.values * input.view(self.vocab_size, 1)).sum(0)

Where do these come from?
--------------------------

.. revealjs_fragments::

   * Trained from a different model
   * Extracted and posted to use
   * (Many more details in NLP class)

Examples
-----------

Embeddings ::

  embedding.weights.value.update(pretrained_weights)

* https://projector.tensorflow.org/

Examples
-----------

Query 1 ::

   ^(lisbon|portugal|america|washington|rome|athens|london|england|greece|italy)$

Query 2 ::

  ^(doctor|patient|lawyer|client|clerk|customer|author|reader)$




Challenge 2 : Sentence Length
==============================


Sentence Length
-----------------

.. revealjs_fragments::

   * Examples may be of different length
   * Need to all be converted to vectors and utilized


Challenge: Length Dimension
------------------------------

.. image:: figs/NLP/senthot.png
           :align: center

Value Transformation
---------------------

.. revealjs_fragments::

   * batch x length x vocab
   * batch x length x feature
   * batch x feature
   * batch x hidden
   * batch

Network
------------------------

.. image:: figs/NLP/sentemb.png
           :align: center


Reduction / "Pooling"
------------------------

.. image:: figs/NLP/pooling.png
           :align: center


Benefits
----------

* Extremely simple
* Embeddings encode key information
* Have all the tools we need


Full Model
------------------------

.. image:: figs/NLP/full.png
           :align: center


Issues
------

* Completely ignores relative order
* Completley ignores absolute order
* Embeddings for all words, even rare ones







Q&A
----
