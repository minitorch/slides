
Module 3.3 - GPU 2
=============================================

Module 3.3
------------

  GPU

Example: Matmul
-------------------
.. image:: figs/Ops/matmul3d1.png
           :align: center

Example: Matmul
-------------------

.. image:: figs/Ops/matmul3d2.png
           :align: center

Computations
-----------------
.. image:: figs/Ops/matmul.png
           :align: center

                   
Matmul Simple
-------------------

.. image:: figs/Ops/matmulsimple.png
           :align: center


Simple Matmul Pseudocode
-------------------------

Code ::

  for outer_index in out.indices():
      for inner_index in range(J):
          out[outer_index] += A[outer_index[0], inner_index] * \
                              B[inner_index, outer_index[1]]

Compare to zip / reduce
-----------------------

Code ::

  ZIP STEP
  C = zeros(broadcast_shape(A.view(I, J, 1), B.view(1, J, K)))
  for C_outer in C.indices():
      C[C_out] = A[outer_index[0], inner_val] * \
                 B[inner_val, outer_index[1]]
  REDUCE STEP
  for outer_index in out.indices():
     for inner_val in range(J):
        out[outer_index] = C[outer_index[0], inner_val,
                             outer_index[1]]


Quiz
-----

Quiz


Today's Class
----------------

.. revealjs_fragments::

   * Threads
   * Memory
   * Communication
 
Threads
========


CUDA Abstraction
-----------------

.. image:: figs/gpu/thread@3x.png
           :align: center


Thread code
---------------

Code ::

  def add(a, b):
    b = a + 10
  cuda_add = numba.cuda.jit()(add)

  cuda_add[1, 1](a, b)


CUDA Abstraction
------------------

.. image:: figs/gpu/blockid@3x.png
           :align: center
           :width: 500px

Block code
---------------

Code ::

  def add(a, b):
    b = a + 10
  cuda_add = numba.cuda.jit()(add)

  cuda_add[(10, 10), (10, 10)](a, b)

Check
-------

Code::

  def printer(a):
    print("hello!")
    a[:] = 10 + 50
  printer = numba.cuda.jit()(printer)
  a = numpy.zeros(10)
  printer[10, 10](a)

Output
--------

Output ::

  hello!
  hello!
  hello!
  hello!
  hello!
  ...

Stack
-------

.. revealjs_fragments::

   * Threads: Run the code
   * Block: Groups "close" threads
   * Grid: All the thread blocks
   * Total Threads: `threads_per_block` x `total_blocks`


Thread Names
==============

CUDA Code
---------

.. revealjs_fragments::

   * Every thread runs simultaneously
   * (Roughly) in lockstep
   * How can we get anything done?

Thread Names
-------------

.. image:: figs/gpu/threadid@3x.png
           :align: center
           :width: 500px

Thread Names
------------

Printing code::

  def printer(a):
    print(numba.cuda.threadIdx.x, numba.cuda.threadIdx.y)
    a[:] = 10 + 50
  printer = numba.cuda.jit()(printer)
  a = numpy.zeros(10)
  printer[1, (10, 10)](a)

Output
------

Output ::

 6 3
 7 3
 8 3
 9 3
 0 4
 1 4
 2 4
 3 4
 4 4

Block Names
-------------

.. image:: figs/gpu/blockid@3x.png
           :align: center
           :width: 500px


Thread Names
------------

Printing code::

  def printer(a):
    print(numba.cuda.blockIdx.x,
          numba.cuda.threadIdx.x, numba.cuda.threadIdx.y)
    a[:] = 10 + 50
  printer = numba.cuda.jit()(printer)
  a = numpy.zeros(10)
  printer[10, (10, 10)](a)

Output
-------

Output ::

  7 6 9
  7 7 9
  7 8 9
  7 9 9
  2 6 9
  2 7 9

What's my name?
---------------

Name ::

  BLOCKS_X = 32
  BLOCKS_Y = 32
  THREADS_X = 10
  THREADS_Y = 10
  def fn(a):
    x = numba.cuda.blockIdx.x * THREADS_X + numba.cuda.threadIdx.x
    y = numba.cuda.blockIdx.y * THREADS_Y + numba.cuda.threadIdx.y
    ...
  fn = numba.cuda.jit()(fn)
  fn[(BLOCKS_X, BLOCKS_Y), (THREADS_X, THREAD_Y)](a)

Simple Map
-----------
Map ::

  BLOCKS_X = 32
  THREADS_X = 32
  def fn(out, a):
    x = numba.cuda.blockIdx.x * THREADS_X + numba.cuda.threadIdx.x
    if x >=0 and x < a.size:
      out[x] = a[x] + 10
  fn = numba.cuda.jit()(fn)
  fn[BLOCKS_X, THREADS_X](a)

Guards
------

Guards ::

  x = numba.cuda.blockIdx.x * BLOCKS_X + numba.cuda.threadIdx.x
  if x >=0 and x < a.size:

Map Guard
----------

.. image:: figs/gpu/map@3x.png
           :align: center
           :width: 500px

Colab
------

* https://colab.research.google.com/drive/1nzH-BHZi-LYK9Ee4t3xvfSr73-qaASwQ#scrollTo=mVmikf3wrekV


Communication
==============


Memory
-------


* CUDA also has a memory stack
* Local > Shared > Global
* Fast code does minimal global reads


Memory is Speed
-----------------

* Faster to read from closer storage.
* Faster to write from closer storage.
  
Local Memory
-------------

.. image:: figs/gpu/local\ mem@3x.png
           :align: center
           :width: 500px


Example
--------

Code ::

  def local_fn(out, a):
    local = numba.cuda.local.array(10, numba.int32)
    local[0] = 10
    local[5] = 20
  local_fn = numba.cuda.jit()(local_fn)
  local_fn[BLOCKS, THREADS](out, a)

Constraints
------------

* Memory must be typed
* Memory must be *constant* size
* Memory must be relatively small


BAD Example
------------

Code ::

  def local_fn(out, a):
    local = numba.cuda.local.array(a.size, numba.int32)
    local[0] = 10
    local[5] = 20
  local_fn = numba.cuda.jit()(local_fn)
  local_fn[BLOCKS, THREADS](out, a)


Block Shared Memory
-------------------

.. image:: figs/gpu/sharedmem@3x.png
           :align: center
           :width: 500px

Example
---------

Code ::

  def block_fn(out, a):
    shared = numba.cuda.shared.array(10, numba.int32)
    shared[0] = 10
    numba.cuda.syncthreads()
    shared[5] = 20
  block_fn = numba.cuda.jit()(block_fn)
  block_fn[BLOCKS, THREADS](out, a)


Colab
------

* https://colab.research.google.com/drive/1nzH-BHZi-LYK9Ee4t3xvfSr73-qaASwQ#scrollTo=mVmikf3wrekV

Real Example
-------------
code ::

  def sum(out, a):
    shared = numba.cuda.shared.array(32, numba.float32)
    i = 32 * numba.cuda.blockIdx.x + numba.cuda.threadIdx.x
    if i < a.size:
        shared[i] = a[i]
        cur = 1
        for i in range(5):
            numba.cuda.syncthreads()
            if i % 2 == 0:
                shared[i // (cur*2)] = shared[i // cur] + \
                                       shared[(i // cur) + 1]
            cur *= 2
    numba.cuda.syncthreads()
    if i == 0:
        out[0] = shared[0]



  
Constraints
------------

* Memory must be typed
* Memory must be *constant* size
* Memory must be relatively small


Algorithms
===========
  

Thinking about Speed
---------------------

* Algorithms: Reduce computation complexity

* Typical: Remove loops, code operations




Sliding Average
-----------------

Compute sliding average over a list ::

   sub_size = 2
   a = [4, 2, 5, 6, 2, 4]
   out = [3, 3.5, 5.5, 4, 3]

Example: Local Sum
--------------------

Compute sliding average over a list ::

    def slide_py(out, a):
       for i in range(out.size):
           out[i] = 0
           for j in range(sub_size):
               out[i] += a[i + j]
           out[i] = out[i] / sub_size


Planning for CUDA
------------------

* Count up the memory accesses

* How many global / shared / local reads?

* Can we make move things to be more local?


Basic CUDA
-----------

Compute CUDA ::

    def slide_cuda(out, a):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           out[i] = 0
           for j in range(sub_size):
                out[i] += a[i + j]
           out[i] = out[i] / sub_size

Planning for CUDA
------------------

* `sub_size` global reads per thread
* `sub_size` global writes per thread

* Each is being read too many times.


Strategy
---------

* Use blocks to move from global to shared
* Use thread to move from shared to local


Better CUDA
-----------

One global write per thread ::

    def slide_cuda(out, a):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           temp = 0
           for j in range(sub_size):
                temp += a[i + j]
           out[i] = temp / sub_size

Pattern
--------

Copy from global to shared ::

     local_idx = numba.cuda.threadIdx.x
     shared[local_idx] = a[i]
     numba.cuda.syncthreads()


Better CUDA
-----------

Two global reads per thread ::

    def slide_cuda(out, a):
       shared = numba.cuda.shared.array(THREADS + sub_size)
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       local_idx = numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           shared[local_idx] = a[i]
           if local_idx < sub_size and i + THREADS < a.size::
               shared[local_idx  + THREADS] = a[i + THREADS]
           numba.cuda.syncthreads()
           temp = 0
           for j in range(sub_size):
                temp += shared[local_idx + j]
           out[i] = temp / sub_size

Counts
-------

* Significantly reduced global reads and writes
* Needed block shared memory to do this

Example 2: Broadcasted Zip
----------------------------

.. image:: figs/Ops/zip\ broad.png


Example 2: Broadcasted Zip
----------------------------

Zip ::

    I, J = out.shape
    def zip_py(out, a, b):
       for i in range(I):
           for j in range(J):
               out[i, j] = a[i] * b[j]



Simple CUDA
----------------------------

Compute CUDA ::

    def zip_cuda(out, a, b):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       j = numba.cuda.blockIdx.y * THREADS \
           + numba.cuda.threadIdx.y

       out[i, j] = a[i] * b[j]

Example 2: Broadcasted Zip
----------------------------

Assume we know that I is very large, J is very small.

.. image:: figs/Ops/zip\ broad.png


Alternative CUDA
-----------------

Compute CUDA (small J, large I) ::

    def zip_cuda(out, a, b):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       shared = numba.cuda.shared.array(J)

       if i < J:
           shared[local_idx] = b[j]

       numba.cuda.syncthreads()
       for j in range(J):
           out[i, j] = a[i] * shared[j]

Matrix Multiplication
----------------------


* Major operation to implement on CUDA
* Every cell needs to be used multiple times
* Lots of tricks to make it efficient...


