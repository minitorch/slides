
Module 3.3 - Advanced GPU
=============================================


Module 3.3
------------

  Advanced GPU



Communication
==============


Memory
-------


* CUDA also has a memory stack
* Local > Shared > Global
* Fast code does minimal global reads

Local Memory
-------------

.. image:: figs/gpu/local\ mem@3x.png
           :align: center
           :width: 500px


Example
--------

Code ::

  def local_fn(out, a):
    local = numba.cuda.local.array(10, numba.int32)
    local[0] = 10
    local[5] = 20
  local_fn = numba.cuda.jit()(local_fn)
  local_fn[BLOCKS, THREADS](out, a)

Constraints
------------

* Memory must be typed
* Memory must be *constant* size
* Memory must be relatively small


BAD Example
------------

Code ::

  def local_fn(out, a):
    local = numba.cuda.local.array(a.size, numba.int32)
    local[0] = 10
    local[5] = 20
  local_fn = numba.cuda.jit()(local_fn)
  local_fn[BLOCKS, THREADS](out, a)


Block Shared Memory
-------------------

.. image:: figs/gpu/sharedmem@3x.png
           :align: center
           :width: 500px

Example
---------

Code ::

  def block_fn(out, a):
    shared = numba.cuda.shared.array(10, numba.int32)
    shared[0] = 10
    numba.cuda.syncthreads()
    shared[5] = 20
  block_fn = numba.cuda.jit()(block_fn)
  block_fn[BLOCKS, THREADS](out, a)


Colab
------

* https://colab.research.google.com/drive/1nzH-BHZi-LYK9Ee4t3xvfSr73-qaASwQ#scrollTo=mVmikf3wrekV

Real Example
-------------
code ::

  def sum(out, a):
    shared = numba.cuda.shared.array(32, numba.float32)
    i = 32 * numba.cuda.blockIdx.x + numba.cuda.threadIdx.x
    if i < a.size:
        shared[i] = a[i]
        cur = 1
        for i in range(5):
            numba.cuda.syncthreads()
            if i % 2 == 0:
                shared[i // (cur*2)] = shared[i // cur] + \
                                       shared[(i // cur) + 1]
            cur *= 2
    numba.cuda.syncthreads()
    if i == 0:
        out[0] = shared[0]


Q&A
----

  
Constraints
------------

* Memory must be typed
* Memory must be *constant* size
* Memory must be relatively small


Thinking about Speed
---------------------

* Algorithms: Reduce computation complexity

* Typical: Remove loops, code operations



GPU
---------------------

* Lots of parallelism for computation

* Challenge: reduce memory reads

* Local > Shared > Global


Sliding Average
-----------------

Compute sliding average over a list ::

   sub_size = 2
   a = [4, 2, 5, 6, 2, 4]
   out = [3, 3.5, 5.5, 4, 3]

Example: Local Sum
--------------------

Compute sliding average over a list ::

    def slide_py(out, a):
       for i in range(out.size):
           out[i] = 0
           for j in range(sub_size):
               out[i] += a[i + j]
           out[i] = out[i] / sub_size

Example: Parallel Local Sum
------------------------------

Compute sliding average over a list ::

    def slide_numba(out, a):
       for i in numba.prange(out.size):
           out[i] = 0
           for j in range(sub_size):
               out[i] += a[i + j]
           out[i] = out[i] / sub_size

Planning for CUDA
------------------

* Count up the memory accesses

* How many global / shared / local reads?

* Can we make move things to be more local?


Basic CUDA
-----------

Compute CUDA ::

    def slide_cuda(out, a):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           out[i] = 0
           for j in range(sub_size):
                out[i] += a[i + j]
           out[i] = out[i] / sub_size

Planning for CUDA
------------------

* `sub_size` global reads per thread
* `sub_size` global writes per thread

* Each is being read too many times.


Strategy
---------

* Use blocks to move from global to shared
* Use thread to move from shared to local


Better CUDA
-----------

One global write per thread ::

    def slide_cuda(out, a):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           temp = 0
           for j in range(sub_size):
                temp += a[i + j]
           out[i] = temp / sub_size

Pattern
--------

Copy from global to shared ::

     local_idx = numba.cuda.threadIdx.x
     shared[local_idx] = a[i]
     numba.cuda.syncthreads()


Better CUDA
-----------

Two global reads per thread ::

    def slide_cuda(out, a):
       shared = numba.cuda.shared.array(THREADS + sub_size)
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       local_idx = numba.cuda.threadIdx.x
       if i + sub_size < a.size:
           shared[local_idx] = a[i]
           if local_idx < sub_size and i + THREADS < a.size::
               shared[local_idx  + THREADS] = a[i + THREADS]
           numba.cuda.syncthreads()
           temp = 0
           for j in range(sub_size):
                temp += shared[local_idx + j]
           out[i] = temp / sub_size

Counts
-------

* Significantly reduced global reads and writes
* Needed block shared memory to do this

Example 2: Broadcasted Zip
----------------------------

.. image:: figs/Ops/zip\ broad.png


Example 2: Broadcasted Zip
----------------------------

Zip ::

    I, J = out.shape
    def zip_py(out, a, b):
       for i in range(I):
           for j in range(J):
               out[i, j] = a[i] * b[j]



Simple CUDA
----------------------------

Compute CUDA ::

    def zip_cuda(out, a, b):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       j = numba.cuda.blockIdx.y * THREADS \
           + numba.cuda.threadIdx.y

       out[i, j] = a[i] * b[j]

Example 2: Broadcasted Zip
----------------------------

Assume we know that I is very large, J is very small.

.. image:: figs/Ops/zip\ broad.png


Alternative CUDA
-----------------

Compute CUDA (small J, large I) ::

    def zip_cuda(out, a, b):
       i = numba.cuda.blockIdx.x * THREADS \
           + numba.cuda.threadIdx.x
       shared = numba.cuda.shared.array(J)

       if i < J:
           shared[local_idx] = b[j]

       numba.cuda.syncthreads()
       for j in range(J):
           out[i, j] = a[i] * shared[j]

Matrix Multiplication
----------------------


* Major operation to implement on CUDA
* Every cell needs to be used multiple times
* Lots of tricks to make it efficient...


General Efficiency
==================

Pedagogy
---------

.. revealjs_fragments::

   * Module-3 will require some optimizations
   * These are up to you to implement.
   * Try first on CPU



Suggestions: Map
-----------------

.. revealjs_fragments::

   * When do you need to index?
   * When do you need to broadcast?
   * Can you directly utilize storage?


Reduce
========
     

Suggestions: Reduce
-------------------

.. revealjs_fragments::

   * Special cases: dimension reduce + full reduce, how do they differ?
   * Do we need to call index everytime?
   * Do we need to write to global memory every time?
