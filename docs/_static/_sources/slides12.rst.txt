
.. raw:: html

   <link rel="stylesheet" href="_static/revealjs/css/theme/white.css">
   <link rel="stylesheet" href="_static/default.css">


Machine Learning Engineering
=============================================


Lecture 12
------------

   Tensor Functions


Lecture
========

Outline
--------

.. revealjs_fragments::

   * Review: Views / Strides
   * Tensor Functions
   * Operations
   * Gradients


Views / Strides
================

User API
------------

.. revealjs_fragments::

   * Dims - # dimensions (`tensor.dims`)
   * Shape -  # cells per dimension (`tensor.shape`)
   * Size - # cells (`tensor.size`)

User Counting
-------------

Shape = (2, 3, 2)


Index Counting

.. revealjs_fragments::

   * (0, 0, 0)
   * (0, 0, 1)
   * (0, 1, 0)

Shape Maniputation
------------------

* Permutation ::

    tensor.permute(1, 0)

.. image:: figs/Tensors/matrix1.png
.. image:: figs/Tensors/matrix2.png

How does this work
--------------------

* **Storage** :  1-D array of numbers of length `size`

* **Strides** : tuple that provides the mapping from user `indexing`
  to the `position` in the 1-D `storage`.


Strides
--------------

.. image:: figs/Tensors/stride1.png
           :align: center
           :width: 400px

Stride Math
------------

Calculating from strides  ::

  stride1 * index1 + stride2 * index2 + stride3 * index3 ...


Lecture Quiz
-------------

  `Quiz <https://canvas.cornell.edu/courses/20583/assignments/163583>`_


Tensor Functions
================

Functions
----------

* Moving from Scalar to Tensor Functions

* Implementation? ::

    def add2(a, b):
       out_tensor = minitorch.zeros(*a.shape)
       for i in range(a.shape[0]):
           for j in range(a.shape[1]):
              out_tensor[i , j] = a[i, j] + b[i, j]
       return out_tensor

Issues
----------

.. revealjs_fragments::

   * Different code per different dims
   * Big autodiff graph
   * Slow, lots of Python loops
   * Lots of code

Tensor Functions
-----------------

.. revealjs_fragments::

   * Tensors as Variables.
   * Track graph at tensor level
   * Functions wrap / unwrap Tensors ::

       out = a + b

Implementation
---------------

.. revealjs_fragments::
   * `Function` class (forward / backward)
   * Similar API as scalars
   * Take / return Tensor objects

Operations
================

Implementing Tensor Functions
------------------------------

.. revealjs_fragments::

   * Option: code `for` loop for each
   * Lazy. We did this already...
   * Optimization. how do we make it fast?

Strategy
-----------

.. revealjs_fragments::

   * Implement high-level functions
   * `Lift` scalar operators to tensors
   * Go back and optimize high-level functions
   * Customize important Functions

Tensor Functions
-----------------

Unary ::

    new_tensor = tensor.log()

Binary (for now, only same shape) ::

    new_tensor = tensor1 + tensor2

Reductions ::

    new_tensor = tensor.sum()


Tensor Ops
-----------------

.. revealjs_fragments::

   1) **Map** - Apply to all elements

   2) **Zip** (same as zipWith) - Apply to all pairs

   3) **Reduce** - Reduce a subset


Map
---------

.. image:: figs/Ops/map.png
           :align: center

Examples: Map
-------------

Binary operations ::

    new_tensor = tensor1.log()
    new_tensor = tensor1.exp()
    new_tensor = -tensor1


Zip
---------

.. image:: figs/Ops/zip.png
           :align: center

Examples: Zip
-------------

Binary operations ::

    new_tensor = tensor1 + tensor2
    new_tensor = tensor1 * tensor2
    new_tensor = tensor1 < tensor2


Reduce
---------

.. image:: figs/Ops/reduce.png
           :align: center

Reduce Options
---------------

.. revealjs_fragments::

   * Can reduce full tensor

   * Can also just reduce 1 dimension ::

       out = minitorch.rand(3,4,5).mean(1)
       print(out.shape )
       # (3, 1, 5)

Examples: Reduce
-------------------

Binary operations ::

    new_tensor = tensor1.mean()
    new_tensor = tensor1.sum(1)


Notes
------

.. revealjs_fragments::

   * None of this relies on strides
   * However, you will need strides to implement


Gradients
==========

Derivatives
------------

.. revealjs_fragments::

   * Each tensor arg is many args
   * Returning tensor is like running several Functions
   * Result `backward` needs to run chain-rule for each arg and output.


Terminology
------------

.. revealjs_fragments::

   * Scalar -> Tensor
   * Derivative -> Gradient
   * `d_out` ->  `grad_out`
   * Recommendation: Reason through gradients as many derivatives


Map Gradient
------------

.. image:: figs/Ops/map\ back.png
           :align: center

Zip Gradient
------------

.. image:: figs/Ops/zip\ back.png
           :align: center


Reduce Gradient
----------------


.. image:: figs/Ops/reduce\ back.png
           :align: center



Q&A
----
