

Module 1.2 - Autodifferentiation
=================================


Module 1.2
------------

  Autodifferentiation

Symbolic Derivative
---------------------

.. math ::

   f(x) = \sin(2 x)


.. math ::

    f'(x) = 2 \times \cos(2 x)

* `Differentiation Rules <https://en.wikipedia.org/wiki/Differentiation_rules>`_


Numerical Derivative
----------------------

.. math ::

    f'(x) \approx  \frac{f(x + \epsilon) - f(x-\epsilon)}{2\epsilon}

.. image:: figs/Grad/approx.png
           :align: center



Autodifferentiation Goals
----------------------------

.. revealjs_fragments::

   * Write down arbitrary (python) functions
   * Programmatically compute the derivative 
   * Two passes: run function, compute derivative

Box Diagrams
-------------

.. image:: figs/Autograd/autograd1.png
           :align: center

     
Box Diagrams
-------------

.. image:: figs/Autograd/autograd2.png
           :align: center


                   

Lecture Quiz
-------------

  `Quiz`


Outline
---------

.. revealjs_fragments::

   * Variables and Functions
   * Backward
   * Chain Rule
   * Backpropagation


Autodifferentiation
--------------------

* Two pass strategy

* Forward Pass - Compute arbitrary function

* Backward Pass - Compute derivatives of function

Forward Pass
--------------------

* Run mathematical code

* Collects result and computation graph

.. image:: figs/Autograd/backprop1.png
           :align: center

  
Backward Pass
--------------------

* Uses computation graph to compute derivatives

* Happens internally in our system. 

.. image:: figs/Autograd/backprop7.png
           :align: center


Example : Linear Model
-----------------------

* Our forward computes 

.. math::
   {\cal L}(w, b) = - \log \sigma(x;w, b)

where

.. math::
   m(x; w, b) = x_1 \times w_1 + x_2 \times w_2 + b

* Our backward computes

.. math::
  {\cal L}'_w(w, b) \ \   {\cal L}'_b(w, b)



  
     
Functions and Variables
=======================



Strategy
-----------

Need to collect all the computation

.. revealjs_fragments::

  1. Replace numbers with  `Variables`.
  2. Replace mathematical function with `Functions`.
  3. `Variables` track which `Functions` were applied

"Wrapping"
----------

* Class wraps around numbers ::

      class Scalar(Variable):
          def __init__(self, value):
               self.value = value
          ...

Goal
-----

.. revealjs_fragments::

   * Act like a numerical value to user
   * Record operations applied
   * Hide access to internal storage



Functions
-----------

.. revealjs_fragments::

   * Functions are implemented as static classes
     
   * We implement hidden `forward` and `backward` methods

   * User calls `apply` which handles wrapping / unwrapping 
   

     

Functions
-----------

* Function :math:`f(x) = x \times 5`

* Implementation ::

      class TimesFive(ScalarFunction):

          @staticmethod
          def forward(ctx, x):
              return x * 5


* :math:`x` is unwrapped (python number) and return is a number

Multi-arg Functions
----------------------

Code ::

      class Mul(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x * y

.. image:: figs/Autograd/autograd2.png
           :align: center

Python Details
------------------

* Use `apply` for the above Functions ::

   x = minitorch.Scalar(10., name="x")
   y = minitorch.Scalar(5., name="y")
   z = TimesFive.apply(x)
   out = TimesFive.apply(z)

* Apply unwraps, calls, and wraps again

Tricks
------------------

* Use operator overloading to ensure that functions are called ::

    out2 = x * y

* Many functions e.g. `sub` can be implemented with other calls.


Notes
------------------

* Since each operation creates a new variable, there are no loops.

* Cannot modify a variable! Graph only gets larger. 

Backwards
==========

How do we get derivatives?
--------------------------

* Base case: compute derivatives for single functions

* Inductive case: define how to propagate a derivative 


  
Base Case: Coding Derivatives
-----------------------------

* For each :math:`f` we need to also provide :math:`f'`
* This part can be done through local symbolic or even numeric differentiation

.. image:: figs/Autograd/autograd3.png
           :align: center



Code
------

* Backward use :math:`f'`
* Returns :math:`f'(x) \times d_{out}`

 ::

      class TimesFive(ScalarFunction):
          @staticmethod
          def forward(ctx, x):
              return x * 5

          @staticmethod
          def backward(ctx, d_out):
              f_prime = 5
              return f_prime * d_out

Picture
----------

.. image:: figs/Autograd/autograd3.png
           :align: center

Code
----------

* What about :math:`f(x, y)`
* Returns :math:`f'_x(x,y) \times d_{out}` and  :math:`f'_y(x,y) \times d_{out}`::


      class AddTimes2(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x + 2 * y

          @staticmethod
          def backward(ctx, d_out):
              return d_out, 2 * d_out

What is Context?
------------------

* Promise -> context on `forward` is given to `backward`

* May be called at different times. 
              
              
Handling Variables
--------------------

Consider a function `Square`

* :math:`g(x) = x^2` that squares x
* Derivative function uses variable :math:`g'(x) = 2 \times x`
* However backward doesn't take args ::

    def backward(ctx, d_out):

Context
----------

Arguments to backward must be saved in context. ::

  class Square(ScalarFunction):
      @staticmethod
      def forward(ctx, x):
          ctx.save_for_backward(x)
          return x * x

      @staticmethod
      def backward(ctx, d_out):
          x = ctx.saved_values
          f_prime = 2 * x
          return f_prime * d_out

Context Internals
-----------------

Run `Square` ::

   x = minitorch.Scalar(10)
   x_2  = Square.apply(x)
   x_2.history.context

              

