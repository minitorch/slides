

Module 2.3 - Tensor Functions
=============================================



Lecture 13
------------

   Broadcasting


Morning Session
=================

Errata
-------------------

* Tasks
* Bad Assert


Reduction
---------
.. image:: figs/Reduce/sum0.png
           :align: center

Reduction
---------

.. image:: figs/Reduce/sum1.png
           :align: center

Reduction
---------

.. image:: figs/Reduce/implement.png
           :align: center


Reduction
---------

.. image:: figs/Reduce/sum2.png
           :align: center


Lecture
========

Outline
--------

.. revealjs_fragments::

   * Review: Gradients
   * Challenges
   * Broadcasting

Review: Gradients
===================


Terminology
------------

.. revealjs_fragments::

   * Scalar -> Tensor
   * Derivative -> Gradient
   * `d_out` ->  `grad_out`
   * Recommendation: Reason through gradients as many derivatives


Map Gradient
------------

.. image:: figs/Ops/map\ back.png
           :align: center

Zip Gradient
------------

.. image:: figs/Ops/zip\ back.png
           :align: center


Reduce Gradient
----------------


.. image:: figs/Ops/reduce\ back.png
           :align: center


Lecture Quiz
-------------

  `Quiz <https://canvas.cornell.edu/courses/20583/assignments/164632>`_



Broadcasting
==============

High Level
---------------

.. revealjs_fragments::

   * Apply same operation multiple times
   * Avoid loops and writes
   * Save memory


First Challenge
---------------

.. revealjs_fragments::

   * Relaxing Zip constraints
   * Apply zip without shapes being identical


Motivation: Scalar Addition
------------------------------

.. math::

   vector1 + 10

Naive Scalar Addition 1
-----------------------

* Repeat vector-size

.. math::

   vector1 + [10, 10, 10]

.. code::

  vector1 + tensor([10, 10, 10])

Naive Scalar Addition2
----------------------

* Write a `for` loop

.. code::

  temp_vector = zeros(vector1.shape)
  for i in vector.shape[0]:
      temp_vector[i] = vector1[i] + 10




Broadcasting
============

Broadcasting
------------

.. revealjs_fragments::

   * No intermediate terms
   * Define rules to make different shapes work together
   * Avoid for loops entirely


Zip With Broadcasting
---------------------


.. image:: figs/Ops/zip\ broad\ back.png

Zip With Broadcasting
---------------------

Code ::

  out = zeros(3, 2)
  for i in range(3):
      for j in range(2):
          out[i, j] = a[i] + b[j]

Zip Back Broadcasting
---------------------

.. image:: figs/Ops/zip\ broad.png


Rules
-----

*   **Rule 1**: Dimension of size 1 broadcasts with anything
*   **Rule 2**: Extra dimensions of 1 can be added with `view`
*   **Rule 3**: Zip automatically adds starting dims of size 1


Matrix Scalar Addition
-----------------------

Matrix + Scalar ::

    matrix1 + tensor([10])

Matrix Scalar Addition
-----------------------

Matrix + Vector ::

    matrix1.view(4, 3) + tensor([1, 2, 3])

Matrix Scalar Addition
-----------------------

Doesn't Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5])

Does Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5]).view(4, 1)

Applying the Rules
-------------------


* (3, 4, 5) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 5) => X

Exercises
-------------------

* (1, 3, 4) | (1, 3, 1) => ?
* (1, 4, 4) | (3, 1, 5) => ?
* (3, 4, 1) | (1) => ?


Examples
=========

Tensor-Scalar operations
------------------------

.. image:: figs/Broadcast/scalar.png
           :align: center

Matrix-vector operations
-------------------------


.. image:: figs/Broadcast/vector.png
           :align: center

Matrix-matrix operations
---------------------------

.. image:: figs/Broadcast/threed.png
           :align: center

(Transposed) Matrix multiplication
----------------------


.. image:: figs/Broadcast/matmul.png
           :width: 600px
           :align: center

Autodiff
--------

* Backward automatically broadcast (if needed)


Broadcasting
=============

Implementation
---------------

* Never create an intermediate value.
* Implicit map between output space / input space


Functions
----------

* `shape_broadcast` - create the broadcast dims

* `broadcast_index` - map from broadcasted to original value



Q&A
----
