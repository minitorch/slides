

Module 1.2 - Autodifferentiation
=================================


Module 1.2
------------

  Autodifferentiation

Symbolic Derivative
---------------------

.. math ::

   f(x) = \sin(2 x)


.. math ::

    f'(x) = 2 \times \cos(2 x)

* `Differentiation Rules <https://en.wikipedia.org/wiki/Differentiation_rules>`_


Numerical Derivative
----------------------

.. math ::

    f'(x) \approx  \frac{f(x + \epsilon) - f(x-\epsilon)}{2\epsilon}

.. image:: figs/Grad/approx.png
           :align: center



Autodifferentiation Goals
----------------------------

.. revealjs_fragments::

   * Write down arbitrary (python) functions
   * Programmatically compute the derivative 
   * Two passes: run function, compute derivative

Box Diagrams
-------------

.. image:: figs/Autograd/autograd1.png
           :align: center

     
Box Diagrams
-------------

.. image:: figs/Autograd/autograd2.png
           :align: center


                   

Lecture Quiz
-------------

  `Quiz`


Outline
---------

.. revealjs_fragments::

   * Variables and Functions
   * Backward
   * Chain Rule
   * Backpropagation


Autodifferentiation
--------------------

* Two pass strategy

* Forward Pass - Compute arbitrary function

* Backward Pass - Compute derivatives of function

Forward Pass
--------------------

* Run mathematical code

* Collects result and computation graph

.. image:: figs/Autograd/backprop1.png
           :align: center

  
Backward Pass
--------------------

* Uses computation graph to compute derivatives

* Happens internally in our system. 

.. image:: figs/Autograd/backprop7.png
           :align: center


Example : Linear Model
-----------------------

* Our forward computes 

.. math::
   {\cal L}(w, b) = - \log \sigma(x;w, b)

where

.. math::
   m(x; w, b) = x_1 \times w_1 + x_2 \times w_2 + b

* Our backward computes

.. math::
  {\cal L}'_w(w, b) \ \   {\cal L}'_b(w, b)



  
     
Functions and Variables
=======================



Strategy
-----------

Need to collect all the computation

.. revealjs_fragments::

  1. Replace numbers with  `Variables`.
  2. Replace mathematical function with `Functions`.
  3. `Variables` track which `Functions` were applied

"Wrapping"
----------

* Class wraps around numbers ::

      class Scalar(Variable):
          def __init__(self, value):
               self.value = value
          ...

Goal
-----

.. revealjs_fragments::

   * Act like a numerical value to user
   * Record operations applied
   * Hide access to internal storage



Functions
-----------

.. revealjs_fragments::

   * Functions are implemented as static classes
     
   * We implement hidden `forward` and `backward` methods

   * User calls `apply` which handles wrapping / unwrapping 
   

     

Functions
-----------

* Function :math:`f(x) = x \times 5`

* Implementation ::

      class TimesFive(ScalarFunction):

          @staticmethod
          def forward(ctx, x):
              return x * 5


* :math:`x` is unwrapped (python number) and return is a number

Multi-arg Functions
----------------------

Code ::

      class Mul(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x * y

.. image:: figs/Autograd/autograd2.png
           :align: center

Python Details
------------------

* Use `apply` for the above Functions ::

   x = minitorch.Scalar(10., name="x")
   y = minitorch.Scalar(5., name="y")
   z = TimesFive.apply(x)
   out = TimesFive.apply(z)

* Apply unwraps, calls, and wraps again

Tricks
------------------

* Use operator overloading to ensure that functions are called ::

    out2 = x * y

* Many functions e.g. `sub` can be implemented with other calls.


Notes
------------------

* Since each operation creates a new variable, there are no loops.

* Cannot modify a variable! Graph only gets larger. 

Backwards
==========

How do we get derivatives?
--------------------------

* Base case: compute derivatives for single functions

* Inductive case: define how to propagate a derivative 


  
Base Case: Coding Derivatives
-----------------------------

* For each :math:`f` we need to also provide :math:`f'`
* This part can be done through local symbolic or even numeric differentiation

.. image:: figs/Autograd/autograd3.png
           :align: center



Code
------

* Backward use :math:`f'`
* Returns :math:`f'(x) \times d_{out}`

 ::

      class TimesFive(ScalarFunction):
          @staticmethod
          def forward(ctx, x):
              return x * 5

          @staticmethod
          def backward(ctx, d_out):
              f_prime = 5
              return f_prime * d_out

Picture
----------

.. image:: figs/Autograd/autograd3.png
           :align: center

Code
----------

* What about :math:`f(x, y)`
* Returns :math:`f'_x(x,y) \times d_{out}` and  :math:`f'_y(x,y) \times d_{out}`::


      class AddTimes2(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x + 2 * y

          @staticmethod
          def backward(ctx, d_out):
              return d_out, 2 * d_out

What is Context?
------------------

* Promise -> context on `forward` is given to `backward`

* May be called at different times. 
              
              
Handling Variables
--------------------

Consider a function `Square`

* :math:`g(x) = x^2` that squares x
* Derivative function uses variable :math:`g'(x) = 2 \times x`
* However backward doesn't take args ::

    def backward(ctx, d_out):

Context
----------

Arguments to backward must be saved in context. ::

  class Square(ScalarFunction):
      @staticmethod
      def forward(ctx, x):
          ctx.save_for_backward(x)
          return x * x

      @staticmethod
      def backward(ctx, d_out):
          x = ctx.saved_values
          f_prime = 2 * x
          return f_prime * d_out

Context Internals
-----------------

Run `Square` ::

   x = minitorch.Scalar(10)
   x_2  = Square.apply(x)
   x_2.history.context

              

Chain Rule
===========


Python Details
------------------

* Use `apply` for the above Functions ::

   x = minitorch.Scalar(10.)
   y = minitorch.Scalar(5.)
   z = TimesFive.apply(x)
   out = TimesFive.apply(z)

* Apply unwraps, calls, and wraps again

Chaining Boxes
---------------

Chaining ::

   x = minitorch.Scalar(10., name="x")
   g_x = G.apply(x)
   f_g_x = F.apply(g_x)


.. image:: figs/Autograd/chain1.png
           :align: center


Chain Rule
-----------

* Compute derivative from chain

.. math ::

    f'_x(g(x)) = g'(x) \times f'_{g(x)}(g(x))


.. image:: figs/Autograd/chain2.png
           :align: center


Chain Rule
-----------

.. math::

   \begin{eqnarray*}
   z &=& g(x) \\
   d_{out} &=& f'(z) \\
   f'_x(g(x)) &=&  g'(x) \times d_{out} \\
   \end{eqnarray*}


.. image:: figs/Autograd/chain2.png
           :align: center

Example: Chain Rule
--------------------

Chaining

.. math::
  
  log(x)^2

.. image:: figs/Autograd/chain1.png
           :align: center

.. math::
   
   f(z) = z^2\\
   g(x) = log(x)\\
                   
Example: Chain Rule
--------------------

.. math::

   f'(z) = 2z \times 1 \\
   g'(x) =  1 / x

   
.. image:: figs/Autograd/chain2.png
           :align: center
                   

What is the combination?

.. math::

   f'_x(g(x))
                   
Two Arguments: Chain
--------------------
.. math::

   \begin{eqnarray*}
  f'_x(g(x, y)) &=& g_x'(x, y) \times f'_{g(x, y)}(g(x, y)) \\
  f'_y(g(x, y)) &=& g_y'(x, y) \times f'_{g(x, y)}(g(x, y))
  \end{eqnarray*}

.. image:: figs/Autograd/chain3.png
           :align: center



Two Arguments: Chain
---------------------

.. math::

   \begin{eqnarray*}
   z &=& g(x, y) \\
   d_{out} &=& f'(z) \\
   f'_x(g(x, y)) &=& g_x'(x, y) \times d_{out} \\
   f'_y(g(x, y)) &=& g_y'(x, y) \times d_{out}
   \end{eqnarray*}

.. image:: figs/Autograd/chain3.png
           :align: center


Example: Chain Rule
--------------------

Chaining

.. math::
  
  (x * y)^2

.. image:: figs/Autograd/chain3.png
           :align: center

.. math::
   
   f(z) = z^2\\
   g(x, y) = (x * y)\\


Example: Chain Rule
--------------------

.. math::

   f'(z) = 2z \times 1\\


What is the combination?

.. math::

   f'_x(g(x, y)) \\
   f'_y(g(x, y)) \\

   
Coding Derivatives
--------------------

* For each :math:`f` or :math:`g`  we need to also provide :math:`f'` and :math:`g'`
* This part can be done through local symbolic or numeric differentiation

.. image:: figs/Autograd/autograd3.png
           :align: center


Picture
----------

.. image:: figs/Autograd/autograd3.png
           :align: center

Code
------

* Backward use :math:`g'`
* Returns :math:`g'(x) \times d_{out}` ::

    class TimesFive(ScalarFunction):
      @staticmethod
      def forward(ctx, x):
         return x * 5

      @staticmethod
       def backward(ctx, d_out):
         g_prime = 5
         return g_prime * d_out


Code
----------

* What about :math:`g(x, y)`
* Returns :math:`g'_x(x,y) \times d_{out}` ::


      class AddTimes2(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x + 2 * y

          @staticmethod
          def backward(ctx, d_out):
              g_prime_x = 1
              g_prime_y = 2
              return g_prime_x * d_out, g_prime_y * d_out

Code
----------

.. image:: figs/Autograd/chain3.png
           :align: center



Backpropagation
=================

Full Graph
-----------

.. math::

   \begin{eqnarray*}
   z &=& x \times y \\
   h(x, y) &=& \log(z) + \exp(z)
   \end{eqnarray*}


.. image:: figs/Autograd/backprop1.png
           :align: center

Method
------

.. revealjs_fragments::

   * Graph propagation
   * `Topological Sorting <http://wikipedia.org/wiki/Topological_sorting>`_
   * Ensure flow to original Variables.

Terminology
------------

.. revealjs_fragments::

   * Leaf: Variable created from scratch
   * Non-Leaf: Variable created with a Function
   * Constant: Term passed in that is not a variable

Algorithm
----------

0. Call topological sort to get an order
1. For each node in order, pull a completed Variable and
   derivative from the queue:

   a. if the Variable is a leaf, add its final derivative (`accumulate_derivative`)
      and loop to (1)
   b. if the Variable is not a leaf,

      1) call `.backprop_step` on the last function that created it with
         derivative as :math:`d_{out}`
      2) loop through all the Variables+derivative produced by the chain
         rule
      3) accumulate derivative for the Variable (check `.unique_id`)

Example
-----------

.. image:: figs/Autograd/backprop1.png
           :align: center

         

Example
-----------

.. image:: figs/Autograd/backprop2.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop3.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop4.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop5.png
           :align: center
Example
-----------

.. image:: figs/Autograd/backprop6.png
           :align: center
Example
-----------

.. image:: figs/Autograd/backprop7.png
           :align: center


Q&A
=====
