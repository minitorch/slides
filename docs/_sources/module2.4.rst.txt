

Module 2.4 - Networks with Tensors
=====================================


Module 2.4
------------

   Applying Neural Networks


Start
=============


Shape Maniputation
------------------

* Permutation ::

    tensor.permute(1, 0)

.. image:: figs/Tensors/matrix1.png
.. image:: figs/Tensors/matrix2.png

Shape Maniputation
------------------

* Permutation ::

    tensor.permute(2, 1, 0, 3)

Rearranges dims in order given.     

           
How does this work
--------------------

* **Storage** :  1-D array of numbers of length `size`

* **Strides** : tuple that provides the mapping from user `indexing`
  to the `position` in the 1-D `storage`.

Tensor Functions
-----------------

Unary ::

    new_tensor = tensor.log()

Binary (for now, only same shape) ::

    new_tensor = tensor1 + tensor2

Reductions ::

    new_tensor = tensor.sum()

Tensor Ops
-----------------

.. revealjs_fragments::

   1) **Map** - Apply to all elements

   2) **Zip** (same as zipWith) - Apply to all pairs

   3) **Reduce** - Reduce a subset


Zip With Broadcasting
---------------------

.. image:: figs/Ops/zip\ broad\ back.png

Zip With Broadcasting
---------------------

Code ::

  out = zeros(3, 2)
  for i in range(3):
      for j in range(2):
          out[i, j] = a[i] + b[j]

Matrix Scalar Addition
-----------------------

Doesn't Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5])

Does Work! ::

    matrix1.view(4, 3) + tensor([1, 2, 3, 5]).view(4, 1)


Applying the Rules
-------------------


* (3, 4, 5) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 1, 5) => (3, 4, 5)
* (3, 4, 1) | (1, 5) => (3, 4, 5)
* (3, 4, 1) | (3, 5) => X


Map Gradient
------------

.. image:: figs/Ops/map\ back.png
           :align: center

Zip Gradient
------------

.. image:: figs/Ops/zip\ back.png
           :align: center


Reduce Gradient
----------------


.. image:: figs/Ops/reduce\ back.png
           :align: center

Zip Broadcasting
---------------------

.. image:: figs/Ops/zip\ broad.png


Example: Negative
------------------------
.. math::

   f(x) = -x

.. image:: figs/Ops/map.png
           :align: center

                   
Example: Negative Back
------------------------

.. math::

     d \times f'(x) = -d


.. image:: figs/Ops/map\ back.png
           :align: center
                   
Example: Inverse
------------------------

.. math::

     f(x) = 1/x


.. image:: figs/Ops/map.png
           :align: center

Example: Inverse Back
------------------------

.. math::

   d \times f'(x) = d \times -x^2 

.. image:: figs/Ops/map\ back.png
           :align: center


Matrix-vector operations
-------------------------


.. image:: figs/Broadcast/vector.png
           :align: center

Backwards (Expand)
-------------------------


.. image:: figs/Broadcast/vector.png
           :align: center

Backwards (Expand)
-------------------------

Rules:

* You can return gradient for broadcasted input.

* Autodiff will collapse it to the original size. 


Chain Rule 
-------------------------

If a variable is used twice, derivative is the use paths. 
  
.. math::

   \begin{eqnarray*}
   z_1 &=& g(x) \\
   z_2 &=& h(x) \\
   f'_x(g(x), h(x)) &=& g'(x) \times f'_{z_1}(z_1, z_2) + h'(x) \times f'_{z_2}(z_1, z_2)
   \end{eqnarray*}
  
                   
Quiz
------

  `Quiz`_  

  
Outline
----------------

.. revealjs_fragments::

   * Training
   * Simple NLP


Training
=========

Reminder
----------

* Dataset - Data to fit
* Model - Shape of fit
* Loss - Goodness of fit

Linear Model Example
----------------------

* Parameters

.. image:: figs/Graphs/weight.png
           :align: center
           :width: 400px


.. image:: figs/Graphs/bias.png
           :align: center
           :width: 400px

Model: Math
------------

.. math::

   \begin{eqnarray*}
   \text{lin}(x; w, b) &=& x_1 \times w_1 + x_2 \times w_2 + b \\
   h_ 1 &=& \text{ReLU}(\text{lin}(x; w^0, b^0)) \\
   h_ 2 &=& \text{ReLU}(\text{lin}(x; w^1, b^1))\\
   m(x_1, x_2) &=& \text{lin}(h; w, b)
   \end{eqnarray*}

                   


Implementation Tricks
----------------------


* Batching -> Compute the loss on many examples.

* Parameters -> Store all in Tensors.

* Opt -> Decide how to change the parameters
  

Batching
--------

.. image:: figs/NLP/batch.png
           :align: center

Batching
--------

* Extra dimension on all the data.

* Will be very important for future assignments

Parameters
-----------

* Tensors inside the model.
  
* Receive the gradient updates

* `Accumumlated` i.e. summed for each update


Optimizer
---------

* Decide how to update the parameters.
  
* Example

  

Parameter Fitting
------------------------------------

1. Compute the loss function, :math:`L(w_1, w_2, b)`
2. See how small changes would change the loss
3. Update to parameters to locally reduce the loss



   
                   
Loss
------

1) Compute Loss ::

    out = model.forward(X).view(data.N)
    loss = -((out * y) + (out - 1.0) * (y - 1.0)).log()




Model: Code
------------

1) Model ::

    class Network(minitorch.Module):
      def __init__(self):
        ...
        self.layer1 = Linear(2, HIDDEN)
        self.layer2 = Linear(HIDDEN, HIDDEN)
        self.layer3 = Linear(HIDDEN, 1)


Layer 1: Weight
----------------

.. image:: figs/NLP/weight.png
           :align: center


Layer 1: Bias
--------------

.. image:: figs/NLP/bias.png
           :align: center

Key Task
---------

.. revealjs_fragments::

   * Use broadcasting to implement the linear function
   * Hint: Align `batch` x `features` x `hidden` to make it work


Layer 2: Weights
----------------

.. image:: figs/NLP/weight2.png
           :align: center


Compute Derivatives
--------------------

Step 2 ::

  (loss.sum().view(1)).backward()
  print(model.layer1.w_1.value.grad)

.. image:: figs/NLP/weight.png
           :align: center


Layer 1: Weight Grad
--------------------

.. image:: figs/NLP/weight.png
           :align: center

.. image:: figs/NLP/weight.png
           :align: center


Update Parameters
--------------------


Step 3 ::

    for p in model.parameters():
        if p.value.grad is not None:
            p.update(p.value - RATE * (p.value.grad / float(data.N)))

Broadcasting
-------------

.. revealjs_fragments::

   * Batches
   * Loss Computation
   * Linear computation
   * Autodifferentiation
   * Gradient updates


Observations
-------------

.. revealjs_fragments::

   * Exactly the same function as Module-1
   * No loops within tensors



